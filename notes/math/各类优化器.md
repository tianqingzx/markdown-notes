@[toc]

#### Adagrad原理

$s_0=0$为梯度平方累计和

$s=\sum{g_x^2}$

$$
\hat{lr}=\frac{\eta}{\sqrt{s+\epsilon}},\quad \epsilon=10^{-10} \\
x=x-g_x\cdot \hat{lr}
$$
这里的$\eta = lr$

#### RMSprop原理

假设目标函数为$L(x,y)=x^2+10y^2$，平滑常数为$\alpha = 0.9$，$\epsilon = 10^{-6}$，$r_x,r_y=0$

计算出梯度为：$g_x,g_y$
$$
累计梯度的平方：&r_x=\alpha r_x+(1-\alpha)(g_x)^2, \quad r_y=... \\
更新参数：&x=x-g_x\cdot \frac{lr}{\sqrt{r_x}+\epsilon}
$$
其中，若$momentum \neq 0$，则：
$$
\bar{r_x}=\bar{r_x}\cdot momentum+\frac{g_x}{\sqrt{r_x}+\epsilon} \\
x=x-lr\cdot \bar{r_x}
$$

#### Momentum SGD原理

$$
v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta_t), \quad \gamma=0.9 \\
\theta_{t+1}=\theta_t-v_t
$$

