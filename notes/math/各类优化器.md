@[toc]

#### Adagrad原理

$s_0=0$为梯度平方累计和

$s=\sum{g_x^2}$

$$
\hat{lr}=\frac{\eta}{\sqrt{s+\epsilon}},\quad \epsilon=10^{-10} \\
x=x-g_x\cdot \hat{lr}
$$
