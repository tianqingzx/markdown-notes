{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parental-distributor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from visdom import Visdom\n",
    "\n",
    "from model import GoogLeNet\n",
    "from mydataset import MyDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-robinson",
   "metadata": {},
   "source": [
    "设置数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "third-brazil",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device.\n"
     ]
    }
   ],
   "source": [
    "num_clazz = 50\n",
    "batch_size = 10\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 4])  # number of workers\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using {} device.\".format(device))\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-browse",
   "metadata": {},
   "source": [
    "viz = Visdom()\n",
    "\n",
    "db = MyDataset('E:\\\\ai_learning_resource\\\\hwdb\\\\HWDB1\\\\train', 224, num_clazz=5)\n",
    "x, y = next(iter(db))\n",
    "print(x.shape)\n",
    "viz.image(db.denormalize(x), win='sample_x', opts=dict(title='sample_x'))\n",
    "loader = DataLoader(db, batch_size=16, shuffle=True)\n",
    "\n",
    "for x, y in loader:\n",
    "    print(x[0].shape)\n",
    "    viz.images(x, nrow=8, win='batch', opts=dict(title='batch'))\n",
    "    viz.text(str(y.numpy()), win='idx', opts=dict(title='batch-y'))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-labor",
   "metadata": {},
   "source": [
    "进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modified-intent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 9514 images for training, 2379 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\22792/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b07a1110fc439a8376994473b74157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/30.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:  1617249840.4630203\n",
      "start train\n",
      "Step 0/4757 \t loss:4.034891605377197\n",
      "Step 400/4757 \t loss:3.9056668281555176\n",
      "Step 800/4757 \t loss:3.9445838928222656\n",
      "Step 1200/4757 \t loss:3.9105424880981445\n",
      "Step 1600/4757 \t loss:3.8826818466186523\n",
      "Step 2000/4757 \t loss:3.8679099082946777\n",
      "Step 2400/4757 \t loss:3.868229866027832\n",
      "Step 2800/4757 \t loss:3.869323253631592\n",
      "Step 3200/4757 \t loss:3.8781254291534424\n",
      "Step 3600/4757 \t loss:3.9354257583618164\n",
      "Step 4000/4757 \t loss:3.9866743087768555\n",
      "Step 4400/4757 \t loss:3.9014649391174316\n",
      "[epoch 1] train_loss: 18635.239  val_accuracy: 0.030\n",
      "start train\n",
      "Step 0/4757 \t loss:3.92185640335083\n",
      "Step 400/4757 \t loss:3.939635753631592\n",
      "Step 800/4757 \t loss:3.9056451320648193\n",
      "Step 1200/4757 \t loss:3.8959262371063232\n",
      "Step 1600/4757 \t loss:3.8929312229156494\n",
      "Step 2000/4757 \t loss:3.979424238204956\n",
      "Step 2400/4757 \t loss:3.8873977661132812\n",
      "Step 2800/4757 \t loss:3.9004693031311035\n",
      "Step 3200/4757 \t loss:3.858065128326416\n",
      "Step 3600/4757 \t loss:3.9139342308044434\n",
      "Step 4000/4757 \t loss:3.9044055938720703\n",
      "Step 4400/4757 \t loss:3.910093307495117\n",
      "[epoch 2] train_loss: 18618.622  val_accuracy: 0.031\n",
      "start train\n",
      "Step 0/4757 \t loss:3.939408779144287\n",
      "Step 400/4757 \t loss:3.9276418685913086\n",
      "Step 800/4757 \t loss:3.940279006958008\n",
      "Step 1200/4757 \t loss:3.923232078552246\n",
      "Step 1600/4757 \t loss:3.949639320373535\n",
      "Step 2000/4757 \t loss:4.0181884765625\n",
      "Step 2400/4757 \t loss:3.9968321323394775\n",
      "Step 2800/4757 \t loss:3.858339786529541\n",
      "Step 3200/4757 \t loss:3.9461464881896973\n",
      "Step 3600/4757 \t loss:3.9229726791381836\n",
      "Step 4000/4757 \t loss:3.9640684127807617\n",
      "Step 4400/4757 \t loss:3.948493242263794\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-52242d2bb8c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# eval model only have last output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0macc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mval_accurate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_db = MyDataset('E:\\\\ai_learning_resource\\\\hwdb\\\\HWDB1\\\\train', 224, num_clazz=num_clazz, mode='train')\n",
    "val_db = MyDataset('E:\\\\ai_learning_resource\\\\hwdb\\\\HWDB1\\\\train', 224, num_clazz=num_clazz, mode='val')\n",
    "# test_db = Pokemon('E:/ai_learning_resource/pokemon/pokemon', 224, mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_db, batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "\n",
    "val_loader = DataLoader(val_db, batch_size=batch_size, num_workers=nw//2)\n",
    "# test_loader = DataLoader(test_db, batch_size=batchsz, num_workers=2)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(len(train_db), len(val_db)))\n",
    "\n",
    "\n",
    "# def evalute(model, loader):\n",
    "#     correct = 0\n",
    "#     total = len(loader.dataset)\n",
    "\n",
    "#     for x, y in loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(x)\n",
    "#             pred = logits.argmax(dim=1)\n",
    "#         correct += torch.eq(pred, y).sum().float().item()\n",
    "#     return correct / total\n",
    "\n",
    "# net = GoogLeNet(num_classes=num_clazz, aux_logits=True, init_weights=True)\n",
    "net = models.densenet121(pretrained=True)\n",
    "net.classifier = nn.Sequential(nn.Linear(1024,256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.2),\n",
    "                                  nn.Linear(256,num_clazz),\n",
    "                                  nn.LogSoftmax(dim=1))\n",
    "\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.classifier.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "criteon = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "start = time.time()\n",
    "print('start time: ', start)\n",
    "# train_steps = len(train_loader)\n",
    "# viz.line([0], [-1], win='loss', opts=dict(title='loss'))\n",
    "# viz.line([0], [-1], win='val_acc', opts=dict(title='val_acc'))\n",
    "# best_acc, best_epoch = 0, 0\n",
    "# global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    print('start train')\n",
    "#     x, y = next(iter(train_loader))\n",
    "#     print(x[0].numpy(), y[0])\n",
    "    running_loss = 0.0\n",
    "#     train_bar = tqdm(train_loader)\n",
    "    for step, data in enumerate(train_loader):\n",
    "#         print('training')\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "#         logits, aux_logits2, aux_logits1 = net(x)\n",
    "        logits = net(x)\n",
    "        loss = criteon(logits, y)\n",
    "#         loss1 = criteon(aux_logits1, y)\n",
    "#         loss2 = criteon(aux_logits2, y)\n",
    "#         loss = loss0 + loss1 * 0.3 + loss2 * 0.3\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if step % 400 == 0:\n",
    "            print('Step {}/{} \\t loss:{}'.format(step, len(train_loader), loss))\n",
    "        \n",
    "#         viz.line([loss.item()], [global_step], win='loss', update='append')\n",
    "#         global_step += 10\n",
    "\n",
    "#         train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, epochs, loss)\n",
    "    \n",
    "    # validate\n",
    "    net.eval()\n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    with torch.no_grad():\n",
    "#         val_bar = tqdm(val_loader)\n",
    "        for val_x, val_y in val_loader:\n",
    "            val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "            outputs = net(val_x)  # eval model only have last output layer\n",
    "            pred_y = outputs.argmax(dim=1)\n",
    "            acc += torch.eq(pred_y, val_y).sum().float().item()\n",
    "\n",
    "    val_accurate = acc / len(val_loader)\n",
    "    scheduler.step()\n",
    "    print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "            (epoch + 1, running_loss, val_accurate))\n",
    "\n",
    "print('Finished Training')\n",
    "print('\\n{} epoch cost time {:f}s'.format(epochs, time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
